{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d1572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# title: osmand_ap_stats.ipynb\n",
    "# gets size of India related osmand download file size over time from archive.org\n",
    "# wip: still to handle html read errors gracefully, currently abort the cell and save the capture_df to file manually\n",
    "# input:  \"data/osmand_ap_stats.csv\"\n",
    "# input: \"data/osmand_ap_captures.csv\"\n",
    "# output: updating_ap_stats, when there is new data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10869349",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import wayback\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc2b66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path 'data/osmand_ap_captures.csv' exists, will not load new captures_data, If updated needed delete the existing file and rerun.\n"
     ]
    }
   ],
   "source": [
    "arx_file_path=Path(\"data/osmand_ap_captures.csv\")\n",
    "if arx_file_path.exists():\n",
    "    print(f\"The path '{arx_file_path}' exists, will not load new captures_data, If updated needed delete the existing file and rerun.\")\n",
    "    arx_df = pd.read_csv(arx_file_path)\n",
    "    arx_df['timestamp']=arx_df['timestamp'].astype('str')\n",
    "    \n",
    "else:\n",
    "    print(f\"The path '{arx_file_path}' does not exist, will create.\")\n",
    "    # check for existence of backups on archive\n",
    "    osmand_downloads_uri= 'download.osmand.net/list.php'\n",
    "    # wayback machine cdx search url \n",
    "    # wayback_url= 'http://web.archive.org/cdx/search/cdx?url='archive.org&from=2010&to=2011'\n",
    "    # get osmand download page captures \n",
    "    wayback_url='https://web.archive.org/cdx/search/cdx?url='+osmand_downloads_uri+'&from=2015&to=2025&output=json'\n",
    "    row_filterexp=r'^India_andhra-pradesh'\n",
    "    # get response\n",
    "    # %%\n",
    "    # Send the request to Overpass API\n",
    "    response = requests.get(wayback_url)\n",
    "\n",
    "    # Check for successful response\n",
    "    if response.status_code != 200:\n",
    "\n",
    "                raise Exception(f\"Wayback machine request failed with status code {response.status_code}\")\n",
    "    else:\n",
    "        print(\"✅ Query executed successfully.\")\n",
    "    \n",
    "        import io\n",
    "        # Convert the JSON response directly to a DataFrame\n",
    "        arx_df = pd.read_json(io.StringIO(response.text))\n",
    "\n",
    "        # 2. Assign the first row to the column headers\n",
    "        new_header = arx_df.iloc[0]  # grabs the first row (index 0)\n",
    "        arx_df.columns = new_header # set the new headers\n",
    "\n",
    "        # 3. Remove the first row from the data\n",
    "        arx_df = arx_df[1:]\n",
    "\n",
    "\n",
    "        # remove rows, where the statuscode is not 200\n",
    "        arx_df= arx_df[arx_df['statuscode'].str.contains('200')]\n",
    "        # 4. (Optional) Reset the index so it starts from 0 again\n",
    "        arx_df = arx_df.reset_index(drop=True)\n",
    "        arx_df['timestamp']=arx_df['timestamp'].astype('str')\n",
    "        # save data so that reattempt is not required\n",
    "        arx_df.to_csv(arx_file_path, index=False, encoding=\"utf-8\")\n",
    "        print(arx_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c62c21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path 'data/osmand_ap_stats.csv' exists, will append any newdata.\n",
      " data to be captured: min_timestamp:20250501000000; max_timestamp:20251207200921\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# The format corresponding to YYYYMMDDHHMMSS, with day set to 01\n",
    "format_pattern = '%Y%m%d%H%M%S'\n",
    "# check for the data that is already written during prior runs\n",
    "file_path=Path(\"data/osmand_ap_stats.csv\")\n",
    "if file_path.exists():\n",
    "    print(f\"The path '{file_path}' exists, will append any newdata.\")\n",
    "    capture_df = pd.read_csv(file_path)\n",
    "    capture_df['File']=capture_df['File'].astype('str')\n",
    "    capture_df['Date']=capture_df['Date'].astype('str')\n",
    "    capture_df['Description']=capture_df['Description'].astype('str')\n",
    "    capture_df.head()\n",
    "    min_ts_str=capture_df['Date'].iloc[-1]\n",
    "    min_ts_yy=min_ts_str[-4:]\n",
    "    min_ts_mm=min_ts_str[3:5]\n",
    "    min_ts=min_ts_yy+min_ts_mm+'01000000'\n",
    "    min_ts_object= datetime.strptime(min_ts, format_pattern)+relativedelta(months=3)\n",
    "    min_ts=min_ts_object.strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "else:\n",
    "    #initialise capture_df to gather relevant data from archive captures.\n",
    "\n",
    "    capture_df = pd.DataFrame({'File': pd.Series(dtype='str'),\n",
    "                'Date': pd.Series(dtype='str'),\n",
    "                'Size': pd.Series(dtype='float'),\n",
    "                'Description': pd.Series(dtype='str')\n",
    "                })\n",
    "    print(f\"The path '{file_path}' does not exist, will create if I get new data.\")\n",
    "\n",
    "if 'min_ts' not in locals():\n",
    "    min_ts=arx_df['timestamp'][0]\n",
    "max_ts=arx_df['timestamp'].iloc[-1]\n",
    "\n",
    "print(f' data to be captured: min_timestamp:{min_ts}; max_timestamp:{max_ts}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74cab56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datetime object: 2025-05-01 00:00:00\n",
      "Type: <class 'datetime.datetime'>\n"
     ]
    }
   ],
   "source": [
    "# Convert the string to a datetime object\n",
    "# only retain year and month\n",
    "min_ts=min_ts[0:6]+'01000000'\n",
    "min_ts_object= datetime.strptime(min_ts, format_pattern)\n",
    "max_ts_object= datetime.strptime(max_ts, format_pattern)\n",
    "\n",
    "# Print the resulting datetime object and its type\n",
    "print(f\"Datetime object: {min_ts_object}\")\n",
    "print(f\"Type: {type(min_ts_object)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db5effc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking archived page for ^202505\n",
      "The index of the last match is: 168\n",
      "https://web.archive.org/web/20250502001157id_/http://download.osmand.net/list.php\n",
      "✅ Query executed successfully.\n",
      "record(s) added ^202505\n",
      "Checking archived page for ^202508\n",
      "The index of the last match is: 174\n",
      "https://web.archive.org/web/20250823105239id_/http://download.osmand.net/list.php\n",
      "✅ Query executed successfully.\n",
      "record(s) added ^202508\n",
      "Checking archived page for ^202511\n",
      "The index of the last match is: 179\n",
      "https://web.archive.org/web/20251126025324id_/http://download.osmand.net/list.php\n",
      "✅ Query executed successfully.\n",
      "record(s) added ^202511\n"
     ]
    }
   ],
   "source": [
    "from requests.exceptions import HTTPError, RequestException\n",
    "# \n",
    "# Path to your CSV file (change 'data.csv' to your file name or path)\n",
    "# capture_df=pd.DataFrame()  already initiated in the first cell\n",
    "# 'MS' frequency generates the start of each month (Month Start)\n",
    "for dt in pd.date_range(start=min_ts_object, end=max_ts_object, freq='3MS'):\n",
    "    dt_yymm=dt.strftime(\"%Y%m\")\n",
    "    check_ts=dt_yymm+'01000000'\n",
    "    dt_pattern=r'^'+dt_yymm\n",
    "    print(f'Checking archived page for {dt_pattern}') \n",
    "    # get the first timestamp in df ['timestamp] that contains dt_str\n",
    "    # 1. Create a boolean Series indicating which rows match the regex\n",
    "    matches=arx_df.index[arx_df['timestamp'].str.contains(dt_pattern,regex=True,na=False)]\n",
    "    # 2. Find the index of the last True value\n",
    "    # Get all indices where the value matches\n",
    "\n",
    "    # The last matching index is the last element of the resulting Index object\n",
    "    if len(matches)==0:\n",
    "        print(f\"capture for timestamp not available, move to next\")\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        # process webpages\n",
    "        # get last match index \n",
    "        last_match_index = matches[len(matches)-1]\n",
    "        print(f\"The index of the last match is: {last_match_index}\")\n",
    "\n",
    "        capture_ts = arx_df.loc[last_match_index]['timestamp']\n",
    "        capture_url= arx_df.loc[last_match_index]['original']\n",
    "        wayback_url='https://web.archive.org/web/'+capture_ts+'id_/'+capture_url\n",
    "        print(wayback_url)\n",
    "        # get captured page\n",
    "        attempt=1 \n",
    "        while True:\n",
    "            try:\n",
    "                response = requests.get(wayback_url, timeout=10) # Set a timeout to prevent hanging\n",
    "                response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "\n",
    "                # If no exception was raised, the request was successful\n",
    "                print(\"✅ Query executed successfully.\")\n",
    "                # Parse the HTML content using BeautifulSoup\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                table = soup.find(\"table\")\n",
    "                page_df = pd.read_html(io.StringIO(str(table)))[0]\n",
    "                #check if india related file rows exist\n",
    "                page_df=page_df[page_df['File'].str.contains(r'^India_andhra-pradesh',case=False, regex=True)]\n",
    "\n",
    "                if len(page_df)>0 :\n",
    "                    capture_df = pd.concat([capture_df, page_df], ignore_index=True)\n",
    "                    print(f\"record(s) added {dt_pattern}\")\n",
    "                break\n",
    "\n",
    "                if(attempt==3):\n",
    "                    break\n",
    "                attempt=attempt+1\n",
    "                time.sleep(5)  # wait for 5 seconds before retrying\n",
    "            except HTTPError as http_err:\n",
    "                print(f\"HTTP error occurred: {http_err}\") # e.g., 404 Not Found, 503 Service Unavailable\n",
    "\n",
    "            except RequestException as req_err:\n",
    "                print(f\"A general request error occurred: {req_err}\") # e.g., Connection error, Timeout, MissingSchema\n",
    "\n",
    "            except ValueError:\n",
    "                print(\"Successfully received a non-JSON response from the API.\") # Handles cases where the response isn't JSON\n",
    "\n",
    "            except Exception as err:\n",
    "                print(f\"An unexpected error occurred: {err}\") \n",
    "capture_df.to_csv(file_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_df.to_csv(file_path, index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
